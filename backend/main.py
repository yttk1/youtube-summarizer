import os
import re
import json
import asyncio
import traceback
from typing import List, Any, Dict, Optional

from fastapi import FastAPI
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

import httpx
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api._errors import (
    NoTranscriptFound,
    TranscriptsDisabled,
    VideoUnavailable,
    CouldNotRetrieveTranscript,
)
from dotenv import load_dotenv
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]   # thư mục youtube-summarizer
load_dotenv(ROOT / ".env")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

try:
    import yt_dlp
except ImportError:
    yt_dlp = None

# -------------------------
# Configuration
# -------------------------
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
OPENAI_RESPONSES_URL = "https://api.openai.com/v1/responses"
MODEL = "gpt-4o-mini"
MAX_OUTPUT_TOKENS = 10000

# -------------------------
# FastAPI app
# -------------------------
app = FastAPI(title="YouTube Summarizer API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# -------------------------
# Request models
# -------------------------
class AnalyzeRequest(BaseModel):
    url: Optional[str] = None
    text: Optional[str] = None
    source: Optional[str] = None

class ChatRequest(BaseModel):
    url: str
    history: List[Dict[str, str]]

# -------------------------
# Utility: extract video id
# -------------------------
def extract_video_id(url: str) -> str:
    # typical youtube url patterns
    m = re.search(r"v=([A-Za-z0-9_-]{11})", url)
    if m:
        return m.group(1)
    m2 = re.search(r"youtu\.be/([A-Za-z0-9_-]{11})", url)
    if m2:
        return m2.group(1)
    # fallback: maybe user passed id directly
    if re.fullmatch(r"[A-Za-z0-9_-]{11}", url):
        return url
    raise ValueError("Could not extract YouTube video id from URL.")

# -------------------------
# Non-blocking transcript fetch
# -------------------------
def _parse_json3_captions(body: str) -> List[dict]:
    try:
        data = json.loads(body)
    except Exception:
        return []

    events = data.get("events", [])
    out = []
    for ev in events:
        segs = ev.get("segs") or []
        text = "".join(seg.get("utf8", "") for seg in segs).strip()
        if not text:
            continue
        start = float(ev.get("tStartMs", 0)) / 1000.0
        dur = float(ev.get("dDurationMs", 0)) / 1000.0
        out.append({"text": text, "start": start, "duration": dur})
    return out


def _parse_vtt_captions(body: str) -> List[dict]:
    def to_seconds(t: str) -> float:
        # format HH:MM:SS.mmm or MM:SS.mmm
        parts = t.split(":")
        if len(parts) == 2:
            mins, rest = parts
            hrs = 0
        else:
            hrs, mins, rest = parts
        return float(hrs) * 3600 + float(mins) * 60 + float(rest.replace(",", "."))

    entries = []
    blocks = re.split(r"\r?\n\r?\n", body.strip())
    for block in blocks:
        lines = [ln.strip() for ln in block.splitlines() if ln.strip()]
        if len(lines) < 2:
            continue

        times_line = lines[0]
        m = re.search(r"(?P<start>[0-9:.]+)\s+-->\s+(?P<end>[0-9:.]+)", times_line)
        if not m:
            continue
        start_s = to_seconds(m.group("start"))
        end_s = to_seconds(m.group("end"))
        text = " ".join(lines[1:]).strip()
        if not text:
            continue
        entries.append({"text": text, "start": start_s, "duration": max(end_s - start_s, 0.0)})
    return entries


def format_timestamp(seconds: float) -> str:
    """Return hh:mm:ss or mm:ss for a numeric timestamp."""
    try:
        total = int(seconds)
    except Exception:
        return ""
    hrs, rem = divmod(total, 3600)
    mins, secs = divmod(rem, 60)
    if hrs:
        return f"{hrs:02d}:{mins:02d}:{secs:02d}"
    return f"{mins:02d}:{secs:02d}"


def detect_language_hint(text: str) -> str:
    """
    Lightweight detector to decide whether transcript text is likely Vietnamese.
    We look for common accented Vietnamese characters.
    """
    if not text:
        return "english"
    sample = text[:400].lower()
    if re.search(r"[ăâêôơưđáàạảãắằặẳẵấầậẩẫéèẹẻẽóòọỏõốồộổỗớờợởỡúùụủũứừựửữíìịỉĩýỳỵỷỹ]", sample):
        return "vietnamese"
    return "english"


def fetch_transcript_via_yt_dlp(video_id: str, languages: List[str] | None = None) -> List[dict]:
    """
    Fallback transcript fetch using yt-dlp's subtitle extraction when youtube-transcript-api fails.
    Attempts manual captions, then autogenerated captions, and parses JSON3/SRV3/VTT formats.
    """
    if yt_dlp is None:
        raise NoTranscriptFound("yt-dlp is not installed in the environment.")

    lang_candidates = languages or ["vi", "en"]
    # allow broader fallback for language variants
    lang_candidates = list(dict.fromkeys(lang_candidates + [l.split("-")[0] for l in lang_candidates if "-" in l]))
    url = f"https://www.youtube.com/watch?v={video_id}"
    ydl_opts = {"quiet": True, "skip_download": True, "writesubtitles": True, "writeautomaticsub": True}

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(url, download=False)

    subtitles = info.get("subtitles") or {}
    auto_subtitles = info.get("automatic_captions") or {}

    # best effort order: preferred languages, then any available
    def pick_track(caption_dict: Dict[str, Any]) -> Optional[dict]:
        if not caption_dict:
            return None
        # iterate candidate languages first, then all available
        for lang in lang_candidates + list(caption_dict.keys()):
            tracks = caption_dict.get(lang)
            if not tracks:
                continue
            # prefer JSON variants, then VTT, then anything else
            ext_priority = {"json3": 0, "srv3": 1, "vtt": 2}
            sorted_tracks = sorted(tracks, key=lambda t: ext_priority.get(t.get("ext"), 99))
            return sorted_tracks[0]
        return None

    track = pick_track(subtitles) or pick_track(auto_subtitles)
    if not track:
        raise NoTranscriptFound(f"No captions available via yt-dlp for video {video_id}")

    caption_url = track.get("url")
    if not caption_url:
        raise NoTranscriptFound(f"yt-dlp provided an empty caption URL for video {video_id}")

    resp = httpx.get(caption_url, timeout=20.0)
    resp.raise_for_status()

    ext = (track.get("ext") or "").lower()
    if ext in ("json3", "srv3"):
        parsed = _parse_json3_captions(resp.text)
    elif ext == "vtt":
        parsed = _parse_vtt_captions(resp.text)
    else:
        parsed = []

    if not parsed:
        raise NoTranscriptFound(f"Could not parse captions from yt-dlp track ({ext}) for video {video_id}")

    return parsed


def fetch_transcript_sync(video_id: str, languages: List[str] | None = None):
    """
    Try multiple transcript strategies:
    1) preferred languages (manual captions)
    2) autogenerated captions in preferred languages
    3) first available transcript, translated to English if possible (except Vietnamese which we keep)
    """
    api = YouTubeTranscriptApi()
    preferred = languages or ["vi", "en", "en-US", "en-GB"]

    # youtube-transcript-api renamed list_transcripts->list in v1.2.3
    def _list_available_transcripts():
        if hasattr(api, "list_transcripts"):
            return api.list_transcripts(video_id)
        if hasattr(api, "list"):
            return api.list(video_id)
        raise RuntimeError("Incompatible youtube-transcript-api version (missing list/list_transcripts)")

    try:
        transcripts = _list_available_transcripts()
    except NoTranscriptFound as e:
        # let outer layer try yt-dlp as a fallback
        raise e

    # attempt to honor preferred languages
    transcript_obj = None
    try:
        transcript_obj = transcripts.find_transcript(preferred)
    except NoTranscriptFound:
        try:
            transcript_obj = transcripts.find_generated_transcript(preferred)
        except NoTranscriptFound:
            transcript_obj = None

    # fallback: grab the first available transcript
    if not transcript_obj:
        try:
            transcript_obj = next(iter(transcripts))
        except StopIteration as e:
            raise NoTranscriptFound(f"No transcripts available for video {video_id}") from e

    lang_code = (transcript_obj.language_code or "").lower()
    should_translate_to_en = (
        "en" in preferred
        and not lang_code.startswith("en")
        and not lang_code.startswith("vi")
        and getattr(transcript_obj, "is_translatable", False)
    )

    # translate to English when appropriate, but keep Vietnamese transcripts as-is
    if should_translate_to_en:
        try:
            transcript_obj = transcript_obj.translate("en")
        except Exception:
            # keep the original transcript if translation fails
            pass

    raw = transcript_obj.fetch()
    # v1.2.x returns a FetchedTranscript dataclass; older versions return list[dict]
    if hasattr(raw, "to_raw_data"):
        raw = raw.to_raw_data()

    entries = []
    for item in raw:
        if isinstance(item, dict):
            text = item.get("text", "")
            start = float(item.get("start", 0))
            dur = float(item.get("duration", 0))
        else:
            text = getattr(item, "text", "")
            start = float(getattr(item, "start", 0))
            dur = float(getattr(item, "duration", 0))
        if not text:
            continue
        entries.append({"text": text, "start": start, "duration": dur})

    if not entries:
        raise NoTranscriptFound(f"Empty transcript for video {video_id}")

    return entries

async def fetch_transcript(video_id: str, languages: List[str] | None = None):
    loop = asyncio.get_event_loop()
    try:
        return await loop.run_in_executor(None, lambda: fetch_transcript_sync(video_id, languages))
    except TranscriptsDisabled as e:
        # captions might still be retrievable via yt-dlp (auto-captions)
        try:
            return await loop.run_in_executor(None, lambda: fetch_transcript_via_yt_dlp(video_id, languages))
        except Exception as yt_e:
            raise RuntimeError(f"TranscriptsDisabled; yt_dlp_fallback: {yt_e}") from yt_e
    except NoTranscriptFound as e:
        # try yt-dlp fallback before giving up
        try:
            return await loop.run_in_executor(None, lambda: fetch_transcript_via_yt_dlp(video_id, languages))
        except Exception as yt_e:
            raise RuntimeError(f"NoTranscriptFound: {e}; yt_dlp_fallback: {yt_e}") from yt_e
    except VideoUnavailable as e:
        raise RuntimeError("VideoUnavailable") from e
    except CouldNotRetrieveTranscript as e:
        raise RuntimeError(f"CouldNotRetrieveTranscript: {e}") from e
    except Exception as e:
        raise RuntimeError(f"TranscriptFetchFailed: {e}") from e

# -------------------------
# Non-blocking metadata fetch via yt-dlp
# -------------------------
# def fetch_video_metadata_sync(video_id: str) -> Dict[str, Any]:
#     ydl_opts = {"quiet": True, "skip_download": True}
#     with yt_dlp.YoutubeDL(ydl_opts) as ydl:
#         info = ydl.extract_info(video_id, download=False)
#     return {"title": info.get("title"), "channel": info.get("uploader"), "duration": info.get("duration", 0)}

# async def fetch_video_metadata(video_id: str) -> Dict[str, Any]:
#     loop = asyncio.get_event_loop()
#     try:
#         return await loop.run_in_executor(None, lambda: fetch_video_metadata_sync(video_id))
#     except Exception as e:
#         raise RuntimeError(f"MetadataFetchFailed: {e}") from e


# -------------------------
# Metadata via YouTube oEmbed (NO yt-dlp, SAFE, no cookies)
# -------------------------
async def fetch_video_metadata(video_id: str) -> Dict[str, Any]:
    url = f"https://www.youtube.com/oembed?url=https://www.youtube.com/watch?v={video_id}&format=json"

    async with httpx.AsyncClient() as client:
        r = await client.get(url)
        if r.status_code != 200:
            raise RuntimeError(f"MetadataFetchFailed: {r.text}")

    data = r.json()
    return {
        "title": data.get("title"),
        "channel": data.get("author_name"),
        "duration": None  # oEmbed doesn't give duration but it's optional
    }


# -------------------------
# Generic helpers
# -------------------------
def normalize_terms_and_points(payload: dict) -> dict:
    terms = payload.get("terminologies")

    if isinstance(terms, dict):
        payload["terminologies"] = [terms]
    elif terms is None:
        payload["terminologies"] = []
    elif isinstance(terms, str):
        payload["terminologies"] = [{"term": "", "definition": terms}]
    elif not isinstance(terms, list):
        payload["terminologies"] = []

    mp = payload.get("major_points")
    if isinstance(mp, dict):
        payload["major_points"] = [mp]
    elif mp is None:
        payload["major_points"] = []
    elif isinstance(mp, str):
        payload["major_points"] = [{"timestamp": "", "title": "", "summary": mp}]
    elif not isinstance(mp, list):
        payload["major_points"] = []

    chapters = payload.get("chapters")
    if isinstance(chapters, dict):
        payload["chapters"] = [chapters]
    elif chapters is None:
        payload["chapters"] = []
    elif isinstance(chapters, str):
        payload["chapters"] = [{"timestamp": "", "title": "", "summary": chapters}]
    elif not isinstance(chapters, list):
        payload["chapters"] = []

    return payload


def clean_html_to_text(html: str) -> str:
    no_script = re.sub(r"(?is)<script.*?>.*?</script>", " ", html)
    no_style = re.sub(r"(?is)<style.*?>.*?</style>", " ", no_script)
    text_only = re.sub(r"(?s)<[^>]+>", " ", no_style)
    text_only = re.sub(r"\s+", " ", text_only)
    return text_only.strip()


async def fetch_webpage_text(url: str) -> str:
    async with httpx.AsyncClient(follow_redirects=True, timeout=20.0) as client:
        resp = await client.get(url)
        resp.raise_for_status()
    return clean_html_to_text(resp.text)


async def summarize_text_block(text: str, source_label: str = "text", title_hint: Optional[str] = None) -> dict:
    trimmed = (text or "").strip()
    if not trimmed:
        raise ValueError("No text provided for summarization.")

    if len(trimmed) > 20000:
        trimmed = trimmed[:20000]

    system_prompt = (
        "You are an assistant that returns JSON ONLY (no markdown). "
        "Schema keys: title, overview, tags (list of strings), chapters (list of {timestamp,title,summary}), "
        "major_points (list of {timestamp,title,summary}), terminologies (list of {term,definition}), "
        "mindmap (hierarchical list of {title,summary,children}), flashcards (list of {q,a}), "
        "quiz (list of {q,choices,answer}). "
        "Overview: 3-5 dense paragraphs that track the narrative, citing important timestamps in [mm:ss] or [hh:mm:ss] and naming people/events. "
        "Chapters: chronological, 8-14 items with mm:ss or hh:mm:ss timestamps; each has a short title plus 2-3 sentence summary with concrete facts (who/what/why/results). "
        "Major points: 8-12 detailed insights tied to a moment or concept. "
        "Mindmap must be pure JSON (no prose), rooted in a central theme with 4-6 branches; each branch carries a summary and 3-4 children that surface specific ideas/examples. "
        "Keep the output language consistent with the source text."
    )

    user_payload = (
        f"SOURCE_TYPE: {source_label}\n"
        f"TITLE_HINT: {title_hint or ''}\n"
        f"CONTENT:\n{trimmed}\n\n"
        "TASK:\n"
        "- Produce detailed JSON following the schema with descriptive, specific wording (avoid generic filler).\n"
        "- Overview: multi-paragraph timeline-style summary that references timestamps when available.\n"
        "- Chapters: 8-14 items if possible, each with a timestamp/section marker, a concise title, and 2-3 sentence factual summary.\n"
        "- Major_points: 8-12 insights with short titles, timestamps when present, and 2-3 sentence details.\n"
        "- Mindmap: central theme with 4-6 branches; each branch has 3-4 children using the {title, summary, children} shape.\n"
        "- Add clear terminologies, flashcards (6), and quiz questions (5) with 4 choices each.\n"
        "- Return strictly JSON with no extra text."
    )

    resp = await call_openai(
        [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_payload}],
        temperature=0.2,
        max_tokens=MAX_OUTPUT_TOKENS,
    )

    model_text = extract_text_from_responses_api(resp)

    parsed = None
    try:
        parsed = json.loads(model_text)
    except Exception:
        m = re.search(r"\{[\s\S]*\}\s*$", model_text)
        if m:
            candidate = m.group(0)
            try:
                parsed = json.loads(candidate)
            except Exception:
                parsed = None

    if not isinstance(parsed, dict):
        parsed = {"overview": model_text}

    parsed.setdefault("title", title_hint or "Text summary")
    parsed.setdefault("channel", "Custom input")
    parsed.setdefault("type", parsed.get("type", source_label))
    parsed.setdefault("tags", parsed.get("tags", []))
    parsed.setdefault("major_points", parsed.get("major_points", []))
    parsed.setdefault("chapters", parsed.get("chapters", []))
    parsed.setdefault("terminologies", parsed.get("terminologies", []))
    parsed.setdefault("flashcards", parsed.get("flashcards", []))
    parsed.setdefault("quiz", parsed.get("quiz", []))
    parsed.setdefault("mindmap", parsed.get("mindmap", []))
    parsed.setdefault("source", source_label)

    normalize_terms_and_points(parsed)
    return parsed

# -------------------------
# OpenAI Responses API helper (async)
# -------------------------
async def call_openai(messages: list, temperature: float = 0.1, max_tokens: int = MAX_OUTPUT_TOKENS) -> dict:
    if not OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY is not set in environment")
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": MODEL,
        "input": messages,
        "temperature": temperature,
        "max_output_tokens": max_tokens,
    }
    async with httpx.AsyncClient(timeout=120.0) as client:
        r = await client.post(OPENAI_RESPONSES_URL, json=payload, headers=headers)
        try:
            r.raise_for_status()
        except httpx.HTTPStatusError:
            # include response text for debugging
            raise RuntimeError(f"OpenAI API error: {r.status_code} {r.text}")
        return r.json()

# Extract text from Responses API robustly
def extract_text_from_responses_api(resp_json: dict) -> str:
    if not isinstance(resp_json, dict):
        return str(resp_json)
    out = ""
    for item in resp_json.get("output", []):
        if isinstance(item, dict) and item.get("type") == "message":
            for c in item.get("content", []):
                if c.get("type") == "output_text":
                    out += c.get("text", "")
    if out:
        return out
    if "output_text" in resp_json:
        return resp_json["output_text"]
    return json.dumps(resp_json)

# -------------------------
# /api/analyze endpoint
# -------------------------
@app.post("/api/analyze")
async def analyze(req: AnalyzeRequest):
    source = (req.source or "youtube").lower()

    if source in ("text", "long_text", "raw_text"):
        text_input = req.text or req.url
        if not text_input:
            return JSONResponse(
                status_code=400,
                content={"error": "missing_text", "detail": "Text content is required for long text mode."},
            )
        try:
            parsed = await summarize_text_block(text_input, source_label="text")
        except Exception as e:
            return JSONResponse(status_code=500, content={"error": "text_summarize_failed", "detail": str(e)})
        parsed.setdefault("source", "text")
        parsed.setdefault("source_input", text_input)
        return parsed

    if source == "web":
        if not req.url:
            return JSONResponse(
                status_code=400,
                content={"error": "missing_url", "detail": "URL is required for webpage summarization."},
            )
        try:
            page_text = await fetch_webpage_text(req.url)
        except Exception as e:
            return JSONResponse(status_code=500, content={"error": "web_fetch_failed", "detail": str(e)})

        if not page_text:
            return JSONResponse(
                status_code=400,
                content={"error": "empty_page", "detail": "Could not extract readable text from the page."},
            )

        try:
            parsed = await summarize_text_block(page_text, source_label="web", title_hint=req.url)
        except Exception as e:
            return JSONResponse(status_code=500, content={"error": "web_summarize_failed", "detail": str(e)})

        parsed.setdefault("source", "web")
        parsed.setdefault("source_url", req.url)
        parsed.setdefault("source_input", req.url)
        return parsed

    if source in ("video", "audio", "pdf", "file"):
        return JSONResponse(
            status_code=400,
            content={"error": "unsupported_source", "detail": f"Processing for {source} is not implemented yet."},
        )

    if not req.url:
        return JSONResponse(
            status_code=400,
            content={"error": "missing_url", "detail": "YouTube URL is required for this mode."},
        )

    # validate URL / extract id
    try:
        vid = extract_video_id(req.url)
    except Exception as e:
        return JSONResponse(status_code=400, content={"error": "invalid_url", "detail": str(e)})

    # metadata
    try:
        meta = await fetch_video_metadata(vid)
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": "metadata_fetch_failed", "detail": str(e)})

    # transcript
    try:
        transcript_entries = await fetch_transcript(vid, languages=["vi", "en", "en-US", "en-GB"])
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": "transcript_error", "detail": str(e)})

    # Build a compact transcript snippet
    full_text = "\n".join([f"[{format_timestamp(e.get('start', 0))}]{e.get('text','')}" for e in transcript_entries])
    snippet = full_text[:20000]
    language_hint = detect_language_hint(full_text)

    system_prompt = (
        "You are a careful YouTube transcript analyzer. Return JSON ONLY (no markdown, no extra text). "
        "Output must be a single valid JSON object that strictly follows this schema keys: "
        "type (educational|song|other), title, channel, overview, tags, chapters, major_points, terminologies, "
        "mindmap, flashcards, quiz, lyrics, song_analysis. "
        ""
        "General rules: "
        "- Use ONLY information supported by the transcript. Do NOT invent facts, links, sponsors, names, or timestamps. "
        "- If a field cannot be filled from the transcript, use an empty value (\"\", [], or null) and never guess. "
        "- Use the transcript language for ALL text output (prefer Vietnamese if Vietnamese is present, otherwise English). "
        "- Keep wording concise, clear, and specific; avoid generic filler. "
        ""
        "Formatting rules: "
        "- All timestamps must be strings in mm:ss or hh:mm:ss and must be consistent within the output. "
        "- chapters and major_points must be arrays of objects: {timestamp, title, summary}. "
        "- terminologies must be an array of objects: {term, definition, context_timestamp}. "
        "- mindmap must be a hierarchy object: {title, summary, children} where children is an array (can be empty). "
        ""
        "Content requirements: "
        "- Always populate overview, chapters, major_points, terminologies, and mindmap for educational/other content. "
        "- overview: 3-5 robust paragraphs tracing the narrative; weave in [mm:ss]/[hh:mm:ss] timestamps, names, numbers, and causes/effects. "
        "- chapters: 8-14 items, chronological, each with a timestamp, sharp title, and 2-3 sentence summary listing key actions/claims/evidence. "
        "- major_points: 8-15 nuanced insights tied to timestamps; include what is claimed, why it matters, and any numbers/examples. "
        "- tags: 6-12 short tags (no hashtags), derived from transcript terms. "
        "- mindmap: central theme with 4-6 top-level branches; every branch has a summary and 3-4 children containing specific ideas/examples (optionally with timestamps). "
        "- flashcards: 6-12 items as {q, a}; prioritize definitions, key ideas, and contrasts. "
        "- quiz: 6-10 items as {q, options, answer_index, explanation}; options length 4; answer_index is 0-3. "
        ""
        "Song-specific behavior: "
        "- If type == \"song\", populate lyrics and song_analysis. "
        "- lyrics must follow transcript ordering; do not add missing lines. "
        "- song_analysis should include themes, meaning, and notable lines with timestamps if available. "
        "- For type != \"song\", set lyrics to [] and song_analysis to \"\"."
    )


    user_payload = (
        f"LANGUAGE_HINT: {language_hint}\n"
        f"METADATA:\ntitle: {meta.get('title')}\nchannel: {meta.get('channel')}\n\n"
        f"TRANSCRIPT_SNIPPET:\n{snippet}\n\n"
        "TASK:\n"
        "- Classify the video type: educational, song, or other (store in `type`).\n"
        "- Overview: 3-5 timeline-style paragraphs with [mm:ss]/[hh:mm:ss] anchors whenever possible; mention names, claims, and outcomes rather than vague wording.\n"
        "- Chapters: 8-14 chronological items with timestamp, title, and 2-3 sentence factual summary (who/what/why/results), similar to a detailed video timeline.\n"
        "- Major_points: 8-15 nuanced insights (timestamp, title, 2-3 sentence summary) with concrete details and any figures/examples.\n"
        "- Terminologies: 4-8 important terms with concise definitions.\n"
        "- Mindmap: pure JSON tree (no markdown) with a central theme, 4-6 branches, each having 3-4 subpoints using the {title, summary, children} shape; avoid single-word nodes.\n"
        "- Flashcards: 6 Q/A pairs; Quiz: 5 MCQs with 4 choices and `answer` equal to the correct choice text.\n"
        "- If song: include lyrics (if inferable) and song_analysis; non-song fields can be brief but should still respect the schema.\n"
        "- Respond entirely in Vietnamese when LANGUAGE_HINT is Vietnamese; otherwise respond in English.\n"
        "- Return strictly JSON."
    )

    try:
        resp = await call_openai(
            [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_payload}],
            temperature=0.15,
            max_tokens=MAX_OUTPUT_TOKENS,
        )
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": "openai_call_failed", "detail": str(e)})

    model_text = extract_text_from_responses_api(resp)

    # Try to parse JSON from model_text
    parsed = None
    try:
        parsed = json.loads(model_text)
    except Exception:
        # attempt to extract last JSON object in the text
        m = re.search(r"\{[\s\S]*\}\s*$", model_text)
        if m:
            candidate = m.group(0)
            try:
                parsed = json.loads(candidate)
            except Exception as e:
                return JSONResponse(status_code=500, content={"error": "model_json_parse_failed", "detail": str(e), "model_text": model_text[:4000]})
        else:
            return JSONResponse(status_code=500, content={"error": "no_json_in_model_response", "model_text": model_text[:4000]})

    # enrich defaults
    parsed.setdefault("title", meta.get("title"))
    parsed.setdefault("channel", meta.get("channel"))
    parsed.setdefault("overview", parsed.get("overview", ""))
    parsed.setdefault("type", parsed.get("type", "other"))
    parsed.setdefault("tags", parsed.get("tags", []))
    parsed.setdefault("major_points", parsed.get("major_points", []))
    parsed.setdefault("chapters", parsed.get("chapters", []))
    parsed.setdefault("terminologies", parsed.get("terminologies", []))
    parsed.setdefault("mindmap", parsed.get("mindmap", []))
    parsed.setdefault("flashcards", parsed.get("flashcards", []))
    parsed.setdefault("quiz", parsed.get("quiz", []))
    parsed.setdefault("source", "youtube")
    parsed.setdefault("video_id", vid)
    parsed.setdefault("source_input", req.url)

    normalize_terms_and_points(parsed)

    return parsed

# -------------------------
# /api/chat endpoint
# -------------------------
@app.post("/api/chat")
async def chat(req: ChatRequest):
    # reuse the analyze flow to get context
    try:
        analysis = await analyze(AnalyzeRequest(url=req.url))
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": "analysis_failed", "detail": str(e)})

    if isinstance(analysis, JSONResponse):
        return analysis

    context_text = f"Video title: {analysis.get('title')}\nChannel: {analysis.get('channel')}\nOverview: {analysis.get('overview')}\n\nMajor points:\n"
    for p in analysis.get("major_points", [])[:10]:
        context_text += f"- {p.get('timestamp','?')} {p.get('title','')} — {p.get('summary','')}\n"

    messages = [
        {"role": "system", "content": "Answer follow-up questions about the provided video. Use timestamps when relevant."},
        {"role": "system", "content": context_text},
    ]
    for h in req.history:
        messages.append({"role": h.get("role"), "content": h.get("content")})

    try:
        resp = await call_openai(messages, temperature=0.2, max_tokens=MAX_OUTPUT_TOKENS)
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": "openai_chat_failed", "detail": str(e)})

    answer_text = extract_text_from_responses_api(resp)
    return {"answer": answer_text}
